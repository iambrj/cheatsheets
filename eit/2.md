---
author: Bharathi Ramana Joshi
title: Notes on Elements of Information Theory, Chapter 2
---
- Entropy: measure of uncertainty of a random variable $X$
\begin{gather*}
    H(X) = -\sum_{x\in\mathcal{X}} p(x)log\;p(x) = -E\;log\;p(X)
\end{gather*}
observe that it only depends on the probabilities, but not the actual values
taken by $X$ itself
- Base 2 entropy is measured in bits, base $e$ in nats
- Entropy can also be interpreted as the expected value of the function
    log$\frac{1}{p(X)}$
- Lemmas
  1. $H(X)\geq 0$
  2. $H_b(X) = (log_b a)H_a(X)$
- Joint entropy: The joint entropy $H(X, Y)$ of a pair of discrete random
    variables $(X, Y)$ with a joint distribution $p(x, y)$ is defined as
    \begin{gather*}
    H(X, Y) = -\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}} p(x, y) log\;p(x, y)
    = -E\;log\;p(X, Y)
    \end{gather*}
- If $(X, Y)\sim p(x, y)$, the conditional entropy is defined as
    \begin{align*}
    H(Y|X) &= \sum_{x\in\mathcal{X}}p(x)H(Y|X=x)\\
           &= -\sum_{x\in\mathcal{X}} p(x)\sum_{y\in\mathcal{Y}}
           p(y|x)log\;p(y|x)\\
           &=  -\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,
           y)log\;p(y|x)\\
           &= -E_{p(x, y)}log\;p(Y|X)
    \end{align*}
- Theorem : Chain rule
    \begin{gather*}
    H(X, Y) = H(X) + H(Y|X)
    \end{gather*}
- Definition: The **relative entropy/Kullback Leibler distance** between two
    probability mass functions $p(x)$ and $q(x)$ is defined as
    \begin{align*}
    D(p||q) &= \sum_{x\in\mathcal{X}}p(x)log\frac{p(x)}{q(x)}\\
            &= E_plog\frac{p(X)}{q(X)}
    \end{align*}
- Definition: The **mutual information** $I(X; Y)$ of RVs $X$ and $Y$ is the
    relative entropy between the joint distribution and the product distribution
    $p(x)p(y)$
    \begin{align*}
    I(X;Y) &= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x, y)log\frac{p(x, y)}{p(x)p(y)}\\
           &= D(p(x, y) || p(x)p(y))\\
           &= E_{p_{(x,y)}}log\frac{p(X, Y)}{p(X)p(Y)}
    \end{align*}
- Relation between entropy and mutual info:
    \begin{align*}
    I(X;Y) &= H(X) - H(X|Y)\\
           &= H(Y) - H(Y|X)\\
           &= H(X) + H(Y) - H(X, Y)
           &= I(Y;X)
    \end{align*}
- Theorem - Chain rule for entropy : The entropy of a collection of RVs is the
    sum of the conditional entropies
    \begin{gather*}
    H(X_1,\dots,X_n) = \sum_{i = 1}^{n} H(X_i|X_1\dots X_{i-1})
    \end{gather*}
- Definition: The **conditional mutual information** of random variables $X$ and
    $Y$, given $Z$ is given by
    \begin{gather*}
    I(X; Y|Z) = H(X|Z) - H(X|Y, Z)
    \end{gather*}
- Theorem - Chain rule for information:
    \begin{gather*}
    I(X_1,\dots,X_n;Y) = \sum_{i=1}^{n}I(X_i;Y|X_{1},\dots,X_{i-1})
    \end{gather*}
- Definition: The **conditional relative entropy** $D((p(y|x))||(q(y|x)))$ is
    the average of the relative entropies between the conditional probability
    mass functions $p(y|x)$ and $q(y|x)$ over the PMF $p(x)$
    \begin{align*}
    D((p(y|x))||(q(y|x))) &= \sum_x p(x)\sum_y p(y|x)log\frac{p(y|x)}{q(y|x)}
                          &= E_{p(x, y)} log\frac{p(Y|X)}{q(Y|X)}
    \end{align*}
- Theorem: Chain rule for conditional entropy
    \begin{gather*}
    D((p(y|x))||(q(y|x))) = D(p(x)||q(x)) + D(p(y|x)||q(y|x))
    \end{gather*}
- Definition: A function $f(x)$ is said to be **convex** over an interval $(a,
    b)$ if $\forall x_1, x_2\in (a, b)$ and $0\leq\lambda\leq 1$
    \begin{gather*}
    f(\lambda x_1 + (1 - \lambda)x_2)\leq\lambda f(x_1) + (1 - \lambda)f(x_2)
    \end{gather*}
- Definition: A function $f$ is **concave** if $-f$ is convex
- Theorem: If $f$ has a double derivative that is nonnegative (positive)
  everywhere, then it is convex (strictly convex)
- Theorem - Jensen's inequality: If $f$ is a convex function and $X$ is a RV
    then
    \begin{gather*}
    Ef(X)\geq f(EX)
    \end{gather*}
- Theorem - Information inequality: If $p(x)$ and $q(x)$ are PMFs then
    \begin{gather*}
    D(p||q)\geq 0
    \end{gather*}
    with equality iff
    \begin{gather*}
    p(x) = q(x),\forall x
    \end{gather*}
- Corollary - Nonnegativity of mutual information: For any two RVs $X$ and $Y$
    \begin{gather*}
    I(X;Y)\geq 0
    \end{gather*}
    with equality iff they are independent
- Theorem: $H(X)\leq log|\mathbb{X}|$ with equality iff uniform distribution
- Corollary - Conditioning reduces entropy
    \begin{gather*}
    H(X|Y)\leq H(X)
    \end{gather*}
    with equality iff $X$ and $Y$ are independent
- Theorem - Independence bound on entropy
    \begin{gather*}
    H(X_1,\dots,X_n)\leq\sum_{i=1}^{n}H(X_i)
    \end{gather*}
- Theorem - Log sum inequality
    \begin{gather*}
    \sum_{i=1}^n
    a_ilog\frac{a_i}{b_i}\geq(\sum_{i=1}^na)log\frac{\sum_{i=1}^na_i}{\sum_{i=1}^nb_i}
    \end{gather*}
    with equality iff $\frac{a_i}{b_i} = const$
- Theorem: $D(p||q)$ is convex in the pair $(p, q)$
- Theorem - Concavity of entrop: $H(p)$ is the concave function of $p$
- Theorem: If $(X, Y)\sim p(x, y) = p(x)p(y|x)$ then $I(X;Y)$ is concave
    function of $p(x)$ for fixed $p(y|x)$ and convex function of $p(y|x)$ for
    fixed $p(x)$
- Definition: The RVs $X, Y, Z$ form a **Markov chain** (in that order) if the
    conditional distribution of $Z$ depends only on $Y$ and is conditionally
    independent of $X$
    \begin{gather*}
    p(x,y,z) = p(x)p(y|x)p(z|y)
    \end{gather*}
- Theorem - Data processing inequality: If $X\rightarrow Y\rightarrow Z$, then
    $I(X;Y)\geq I(X;Z)$
- Corollaries
    1. $Z = g(Y)\implies I(X;Y)\geq I(X;g(Y))$
    2. $X\rightarrow Y\rightarrow Z\implies I(X;Y|Z)\leq I(X;Y)$
